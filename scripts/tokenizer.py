#!/usr/bin/env python
"""
preprocess_tok.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
One-time script that:
  1. Downloads the raw Arrow shards from S3
  2. Tokenises every sequence with the specified model tokenizer
  3. Saves the tokenised dataset to /opt/dlami/nvme/tok_<model>_max<LEN>

Only rank-0 does the heavy lifting; other ranks just wait and load.
"""

import argparse, os, json
from datasets import load_dataset, load_from_disk
from transformers import AutoTokenizer
from accelerate import Accelerator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
p = argparse.ArgumentParser()
p.add_argument("--model_name", required=True,
               help="HF checkpoint with a compatible tokenizer")
p.add_argument("--s3_root",    required=True,
               help="s3://â€¦ folder that contains test/*.arrow")
p.add_argument("--max_len",    type=int, required=True)
args = p.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Accelerator setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
acc = Accelerator()
rank0 = acc.is_main_process
acc.print = acc.print if rank0 else (lambda *a, **k: None)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Paths & cache dirs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tok_cache = f"/opt/dlami/nvme/tok_{args.model_name.replace('/', '_')}_max{args.max_len}"
os.makedirs(tok_cache, exist_ok=True)   # harmless if already there

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Rank-0 work â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if rank0 and not os.listdir(tok_cache):
    acc.print(f"ğŸ”‘  Loading tokenizer:  {args.model_name}")
    tok = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)

    acc.print("â¬‡ï¸   Loading raw Arrow shards from S3 â€¦")
    raw_ds = load_dataset(
        "arrow",
        data_files={"test": f"{args.s3_root.rstrip('/')}/test/*.arrow"},
        # non-streaming â†’ we need real data to map over
    )["test"]

    def tok_fn(batch):
        return tok(batch["sequence"],
                   truncation=True,
                   max_length=args.max_len)

    acc.print("ğŸ› ï¸   Tokenising â€¦ (this can take a while)")
    (raw_ds
        .map(tok_fn,
             remove_columns=["sequence"],
             batched=True,            # speeds things up
             batch_size=1000,
             num_proc=32,             # g5.12xlarge â†’ 96 vCPUs
             load_from_cache_file=False)
        .save_to_disk(tok_cache))

    acc.print(f"âœ…  Saved tokenised set â†’ {tok_cache}")

# â”€â”€â”€â”€â”€ All other ranks wait for the cache, then exit â”€â”€â”€â”€â”€
acc.wait_for_everyone()
if not rank0:
    acc.print(f"Using cached dataset at {tok_cache}")

# If you want to sanity-check it:
# ds = load_from_disk(tok_cache)
# print(ds[0])
